{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from modeling.deeplab import DeepLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"PyTorch DeeplabV3Plus Inference\")\n",
    "parser.add_argument('--backbone', type=str, default='mobilenet',\n",
    "                    choices=['resnet', 'xception', 'drn', 'mobilenet'],\n",
    "                    help='backbone name (default: resnet)')\n",
    "parser.add_argument('--out-stride', type=int, default=16,\n",
    "                    help='network output stride (default: 8)')\n",
    "parser.add_argument('--nclass', type=int, default=21,\n",
    "                    help='number of classes(incluce background class)')\n",
    "parser.add_argument('--checkpoint', type=str, default=r\"C:\\Users\\CUONG\\Desktop\\model_best.pth.tar\",\n",
    "                    help='Checkpoint file path')\n",
    "parser.add_argument('--size', type=str, default='(513, 513)',\n",
    "                    help='Input size')\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):\n",
    "    model = DeepLab(num_classes=args.nclass,\n",
    "                    backbone=args.backbone,\n",
    "                    output_stride=args.out_stride)\n",
    "    checkpoint = torch.load(args.checkpoint, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_file(model, args, image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = Image.fromarray(img)\n",
    "    img = img.resize(eval(args.size), Image.BILINEAR)\n",
    "    img = np.array(img).astype(np.float32)\n",
    "    img /= 255.0\n",
    "    img -= (0.485, 0.456, 0.406)\n",
    "    img /= (0.229, 0.224, 0.225)\n",
    "    img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
    "    img = torch.from_numpy(img).float()\n",
    "    img = img.unsqueeze(0)\n",
    "    output = model(img)\n",
    "    output = output[0]\n",
    "    predict = torch.argmax(output, dim=0)\n",
    "    predict = predict.cpu().numpy()\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = inference_file(model, args, r\"E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\13961.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[predict > 0] = 255\n",
    "predict = np.stack([np.zeros_like(predict), np.zeros_like(predict), predict], axis=-1)\n",
    "predict = predict.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\13961.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.resize(image, predict.shape[: 2], cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = cv2.addWeighted(image, 0.8, predict, 0.2, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv2.imwrite(r\"E:\\JupyterNotebook\\dataset\\hair\\test.jpg\", blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\0.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\1.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\100.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\1000.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10000.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10001.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10002.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10003.jpg\n",
      "Predicting:  E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\\10004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Warning #96: Cannot form a team with 2 threads, using 1 instead.\n",
      "OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --dir_path \"E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\CelebA-HQ-img\" \\\n",
    "                     --output_dir \"E:\\JupyterNotebook\\dataset\\hair\\CelebAMask-HQ\\output\" \\\n",
    "                     --max_file 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
